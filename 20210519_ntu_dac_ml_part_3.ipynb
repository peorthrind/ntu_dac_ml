{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "opposite-logic",
   "metadata": {},
   "source": [
    "# Machine learning, a tutorial, part III\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-bracelet",
   "metadata": {},
   "source": [
    "## 3-1 Revist tree-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "noticed-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## some setting for better reading experience\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "##  \n",
    "randomState = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "occasional-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read data \n",
    "melb = pd.read_csv(\"data/melb_training_data.csv\").sample(frac=1, random_state=randomState).reset_index(drop=True)\n",
    "melb.drop(columns = [\"Unnamed: 0\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cardiac-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "## select the features\n",
    "features_in_model = [\"Distance\", \"Rooms\", \"YearBuilt\", \"BuildingArea\",\"Landsize\"]\n",
    "X = melb[features_in_model]\n",
    "\n",
    "## set the target\n",
    "y = melb[\"Price\"]\n",
    "\n",
    "## Train / test split\n",
    "X_train = X[:9000]\n",
    "y_train = y[:9000]\n",
    "X_test = X[9000:]\n",
    "y_test = y[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "alternative-object",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the no-feature-engineering data (missing value only):\n",
      "R^2 on training data = 0.9404\n",
      "R^2 on test data = 0.5802\n"
     ]
    }
   ],
   "source": [
    "## Random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators = 300, \n",
    "                                bootstrap = True,\n",
    "                                 oob_score = True,\n",
    "                                 n_jobs = 3,\n",
    "                                 random_state = randomState)\n",
    "\n",
    "print(\"With the no-feature-engineering data (missing value only):\")\n",
    "rf_model.fit(X_train.fillna(-1), y_train)\n",
    "\n",
    "print(\"R^2 on training data = {:.4f}\".format(rf_model.score(X_train.fillna(-1), \n",
    "                                                            y_train)))\n",
    "print(\"R^2 on test data = {:.4f}\".format(rf_model.score(X_test.fillna(-1), \n",
    "                                                        y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "personal-works",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the no-feature-engineering data (missing value only):\n",
      "R^2 on training data = 0.6922\n",
      "R^2 on test data = 0.6052\n"
     ]
    }
   ],
   "source": [
    "## GBDT\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb_model = GradientBoostingRegressor(n_estimators = 300, \n",
    "                                     subsample = 0.7,\n",
    "                                    n_iter_no_change = 10,\n",
    "                                     random_state = randomState)\n",
    "\n",
    "print(\"With the no-feature-engineering data (missing value only):\")\n",
    "gb_model.fit(X_train.fillna(-1), y_train)\n",
    "\n",
    "print(\"R^2 on training data = {:.4f}\".format(gb_model.score(X_train.fillna(-1), \n",
    "                                                            y_train)))\n",
    "print(\"R^2 on test data = {:.4f}\".format(gb_model.score(X_test.fillna(-1), \n",
    "                                                        y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ranging-credit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Distance</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>BuildingArea</th>\n",
       "      <th>Landsize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>6.4000</td>\n",
       "      <td>2</td>\n",
       "      <td>1990.0000</td>\n",
       "      <td>89.0000</td>\n",
       "      <td>312.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Distance  Rooms  YearBuilt  BuildingArea  Landsize\n",
       "9000    6.4000      2  1990.0000       89.0000  312.0000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Take look the first row in test data \n",
    "\n",
    "one_line_test = X_test.head(1).fillna(-1)\n",
    "\n",
    "one_line_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "historic-burns",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicton from RF = 728720.0000\n",
      "Predicton from GBDT = 637323.0770\n",
      "True value = 468000.0000\n"
     ]
    }
   ],
   "source": [
    "## And the predicted values from random forest and GBDT\n",
    "print(\"Predicton from RF = {:.4f}\".format(rf_model.predict(one_line_test)[0]))\n",
    "print(\"Predicton from GBDT = {:.4f}\".format(gb_model.predict(one_line_test)[0]))\n",
    "print(\"True value = {:.4f}\".format(y_test.values[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "delayed-vanilla",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([612500.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Take the first decision tree in the random forest I've trained\n",
    "rf_model.estimators_[0].predict(one_line_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "congressional-indie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "728720.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Take all the prediction results in random forest\n",
    "rf_results = list()\n",
    "\n",
    "for i in range(len(rf_model.estimators_)):\n",
    "    rf_results.append(rf_model.estimators_[i].predict(one_line_test))\n",
    "\n",
    "## The RF take the average of the results from all the trees\n",
    "np.mean(rf_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "threaded-squad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([637323.07695403])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## On the other hand, GBDT add all results up\n",
    "gb_results = list()\n",
    "\n",
    "for i in range(len(gb_model.estimators_)):\n",
    "    gb_results.append(gb_model.estimators_[i][0].predict(one_line_test))\n",
    "\n",
    "## gb_model.init_ is the mean(y) from training data as the 0-th prediction\n",
    "## the learning_rate controls how fast the results are added up\n",
    "gb_model.init_.predict(one_line_test) + np.sum(gb_results)*gb_model.learning_rate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-scratch",
   "metadata": {},
   "source": [
    "## 3-2 Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "occupational-observation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: R^2 on test data = 0.5802\n",
      "GBDT: R^2 on test data = 0.6052\n"
     ]
    }
   ],
   "source": [
    "## If I have only few models, it's ok to use test R2 for choosing the best model\n",
    "\n",
    "print(\"Random Forest: R^2 on test data = {:.4f}\".format(rf_model.score(X_test.fillna(-1), \n",
    "                                                        y_test)))\n",
    "\n",
    "print(\"GBDT: R^2 on test data = {:.4f}\".format(gb_model.score(X_test.fillna(-1), \n",
    "                                                        y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "collect-politics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01 0.07 0.13 0.19 0.25]\n",
      "[ 1  5  9 13 17]\n"
     ]
    }
   ],
   "source": [
    "## I want to find a good set of hyperparameters\n",
    "## Take GBDT for example, the hyperparameters to choose is learning_rate & min_samples_leaf\n",
    "\n",
    "learning_rates = np.arange(0.01, 0.3, 0.06)\n",
    "min_samples_leafs = np.arange(1, 20, 4)\n",
    "\n",
    "## 5 values for both => number of models = 25\n",
    "## this is grid search\n",
    "print(learning_rates)\n",
    "print(min_samples_leafs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cheap-peoples",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0.01, 1): 0.5256142302658419,\n",
       " (0.01, 5): 0.5250243798008423,\n",
       " (0.01, 9): 0.5236738947853083,\n",
       " (0.01, 13): 0.5229067341597174,\n",
       " (0.01, 17): 0.5228485455369929,\n",
       " (0.06999999999999999, 1): 0.6045845572552284,\n",
       " (0.06999999999999999, 5): 0.5693352731126812,\n",
       " (0.06999999999999999, 9): 0.5696395016397571,\n",
       " (0.06999999999999999, 13): 0.5975864041706282,\n",
       " (0.06999999999999999, 17): 0.5940958065635806,\n",
       " (0.12999999999999998, 1): 0.5782171638256237,\n",
       " (0.12999999999999998, 5): 0.5668037781274522,\n",
       " (0.12999999999999998, 9): 0.5758478888745222,\n",
       " (0.12999999999999998, 13): 0.5930712179623087,\n",
       " (0.12999999999999998, 17): 0.5977175520732401,\n",
       " (0.18999999999999997, 1): 0.6002485635061221,\n",
       " (0.18999999999999997, 5): 0.5586668459807924,\n",
       " (0.18999999999999997, 9): 0.6095951724817985,\n",
       " (0.18999999999999997, 13): 0.5877448245871291,\n",
       " (0.18999999999999997, 17): 0.6119960338501851,\n",
       " (0.24999999999999997, 1): 0.6085056003082734,\n",
       " (0.24999999999999997, 5): 0.6018058035253546,\n",
       " (0.24999999999999997, 9): 0.5982254605642026,\n",
       " (0.24999999999999997, 13): 0.5714213494779046,\n",
       " (0.24999999999999997, 17): 0.6062166258595391}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## try all the combinations\n",
    "\n",
    "hyper_searching_results = dict()\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for mnl in min_samples_leafs:\n",
    "        #print(lr, mnl)\n",
    "        gb_model = GradientBoostingRegressor(n_estimators = 300, \n",
    "                                     subsample = 0.7,\n",
    "                                    n_iter_no_change = 10,\n",
    "                                     learning_rate = lr, \n",
    "                                     min_samples_leaf = mnl,\n",
    "                                     random_state = randomState)\n",
    "\n",
    "        gb_model.fit(X_train.fillna(-1), y_train)\n",
    "        R2_test = gb_model.score(X_test.fillna(-1), y_test)\n",
    "        hyper_searching_results[(lr, mnl)] = R2_test\n",
    "        \n",
    "hyper_searching_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fifteen-premises",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.18999999999999997, 17)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the winner goes to ......\n",
    "max(hyper_searching_results, key=hyper_searching_results.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-genesis",
   "metadata": {},
   "source": [
    "### 3-2-1 Cross Validation & Hyperparameter tuning\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "## in many applications, we don't abuse test set like that\n",
    "## we use cross validation to replace multiple evaluations on test set\n",
    "\n",
    "## to do CV, there is no need to write multiple loops all by myself\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters_to_search = {'learning_rate': learning_rates, \n",
    "              'min_samples_leaf': min_samples_leafs}\n",
    "\n",
    "gb_model = GradientBoostingRegressor(n_estimators = 300, \n",
    "                                     subsample = 0.7,\n",
    "                                    n_iter_no_change = 10,\n",
    "                                     random_state = randomState)\n",
    "\n",
    "gb_model_CV = GridSearchCV(gb_model, parameters_to_search, cv=5)\n",
    "gb_model_CV.fit(X_train.fillna(-1), y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the gridcv module run the models and save the results for us\n",
    "gb_model_CV.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dirty-mineral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53843195, 0.53889237, 0.53819574, 0.5381507 , 0.53753945,\n",
       "       0.61588835, 0.61846628, 0.6162798 , 0.61306669, 0.61027594,\n",
       "       0.61261881, 0.61777265, 0.61322641, 0.61834923, 0.61143677,\n",
       "       0.6102079 , 0.62499701, 0.60372491, 0.61588563, 0.60891797,\n",
       "       0.60834103, 0.6177238 , 0.61079874, 0.60410298, 0.61357182])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the mean of 5-folds test(not true test) R2\n",
    "gb_model_CV.cv_results_[\"mean_test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cardiac-acrylic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.18999999999999997, min_samples_leaf=5,\n",
       "                          n_estimators=300, n_iter_no_change=10, random_state=8,\n",
       "                          subsample=0.7)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the best one is learning_rate=0.18999999999999997, min_samples_leaf=5\n",
    "gb_model_CV.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-prefix",
   "metadata": {},
   "source": [
    "## 3-3 Learn from models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-posting",
   "metadata": {},
   "source": [
    "### 3-3-1 Feature importance\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html?highlight=randomforestregressor#sklearn.ensemble.RandomForestRegressor.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "signed-revision",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3028007 , 0.23719954, 0.0910567 , 0.14050044, 0.22844263])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## It's easy to get feature inportance from tree-based models\n",
    "## the values is normalized variation reduction\n",
    "## sum of them is 1\n",
    "\n",
    "rf_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "finite-degree",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Distance', 'Rooms', 'YearBuilt', 'BuildingArea', 'Landsize']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The order corresponds to the features you put in the model\n",
    "features_in_model\n",
    "\n",
    "## so distance is important when predicting house price\n",
    "## but is this equivalent to the answer from linear model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "stupid-guest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear : R^2 on test data = 0.3956\n"
     ]
    }
   ],
   "source": [
    "## for linear models\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train_lr = X_train.copy()\n",
    "X_test_lr = X_test.copy()\n",
    "\n",
    "## missing values\n",
    "mean_YearBuilt = X_train.YearBuilt.mean()\n",
    "mean_BuildingArea = X_train.BuildingArea.mean()\n",
    "\n",
    "X_train_lr[\"YearBuilt\"] = X_train[\"YearBuilt\"].fillna(mean_YearBuilt)\n",
    "X_train_lr[\"BuildingArea\"] = X_train[\"BuildingArea\"].fillna(mean_BuildingArea)\n",
    "\n",
    "X_test_lr[\"YearBuilt\"] = X_test_lr[\"YearBuilt\"].fillna(mean_YearBuilt)\n",
    "X_test_lr[\"BuildingArea\"] = X_test_lr[\"BuildingArea\"].fillna(mean_BuildingArea)\n",
    "\n",
    "\n",
    "## train without any transformation\n",
    "lr_model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "## train the Linear regression model\n",
    "lr_model.fit(X_train_lr, y_train)\n",
    "\n",
    "print(\"Linear : R^2 on test data = {:.4f}\".format(lr_model.score(X_test_lr, \n",
    "                                                        y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "rough-contract",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient for Distance = -32130.3773\n",
      "Coefficient for Rooms = 377407.7353\n",
      "Coefficient for YearBuilt = -4158.6728\n",
      "Coefficient for BuildingArea = 45775923.3946\n",
      "Coefficient for Landsize = 3.8707\n"
     ]
    }
   ],
   "source": [
    "## the coefficents\n",
    "for i in range(len(features_in_model)):\n",
    "    print(\"Coefficient for {} = {:.4f}\".format(features_in_model[i], lr_model.coef_[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "simplified-strategy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear : R^2 on test data = 0.3956\n",
      "Coefficient for Distance = -32130.3773\n",
      "Coefficient for Rooms = 377407.7353\n",
      "Coefficient for YearBuilt = -4158.6728\n",
      "Coefficient for BuildingArea = 45775923394.7389\n",
      "Coefficient for Landsize = 3.8707\n"
     ]
    }
   ],
   "source": [
    "## if the unit of some variables changes, the corresponding coefficient also alters\n",
    "\n",
    "X_train_lr[\"BuildingArea\"] = X_train_lr[\"BuildingArea\"] / 1000\n",
    "X_test_lr[\"BuildingArea\"] = X_test_lr[\"BuildingArea\"] / 1000\n",
    "\n",
    "## train without any transformation\n",
    "lr_model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "## train the Linear regression model\n",
    "lr_model.fit(X_train_lr, y_train)\n",
    "print(\"Linear : R^2 on test data = {:.4f}\".format(lr_model.score(X_test_lr, \n",
    "                                                        y_test)))\n",
    "\n",
    "## the coefficents remain the same except the one for BuildingArea\n",
    "## the one for BuildingArea is 1000x larger\n",
    "for i in range(len(features_in_model)):\n",
    "    print(\"Coefficient for {} = {:.4f}\".format(features_in_model[i], lr_model.coef_[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "efficient-brighton",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear : R^2 on test data = 0.3956\n",
      "Coefficient for Distance = -188170.9616\n",
      "Coefficient for Rooms = 361911.0437\n",
      "Coefficient for YearBuilt = -117549.6360\n",
      "Coefficient for BuildingArea = 21882.1181\n",
      "Coefficient for Landsize = 18586.9919\n"
     ]
    }
   ],
   "source": [
    "## Standardization\n",
    "## https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "## convert x to (x - mean(x)) / sd(x)\n",
    "## the \"unit\" of x is \"one standard deviation from mean\"\n",
    "## so this can be compared between all features\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train_lr)\n",
    "X_train_lr = scaler.transform(X_train_lr)\n",
    "X_test_lr = scaler.transform(X_test_lr)\n",
    "\n",
    "## train the Linear regression model\n",
    "lr_model = LinearRegression(fit_intercept=True)\n",
    "lr_model.fit(X_train_lr, y_train)\n",
    "print(\"Linear : R^2 on test data = {:.4f}\".format(lr_model.score(X_test_lr, \n",
    "                                                        y_test)))\n",
    "\n",
    "## Now the meaning of coefficients is \"the delta of y by 1 standard deviation of x from mean\"\n",
    "## compare the result to feature importance derived from random forest\n",
    "for i in range(len(features_in_model)):\n",
    "    print(\"Coefficient for {} = {:.4f}\".format(features_in_model[i], lr_model.coef_[i]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-tulsa",
   "metadata": {},
   "source": [
    "### 3-3-2 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For tree-based models, you can just drop the features with low importance values\n",
    "## For linear models, you can drop features with the results from tree-base models\n",
    "\n",
    "## unlike the tree-based model, drops \"useless features\" from linear models affects results\n",
    "## you can treat \"feature combination\" just like \"hyperparameter combination\" in a CV manner\n",
    "## however, it's not a \"Grid search\" task\n",
    "## you should use sklearn.model_selection.cross_validate to do this\n",
    "## the grammar are similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-stopping",
   "metadata": {},
   "source": [
    "### 3-4 What is a good model (for you)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-classics",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
