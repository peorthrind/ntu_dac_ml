{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "opposite-logic",
   "metadata": {},
   "source": [
    "# Machine learning, a tutorial, part III\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-bracelet",
   "metadata": {},
   "source": [
    "## 3-1 Revist tree-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "noticed-soccer",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## some setting for better reading experience\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "##  \n",
    "randomState = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "occasional-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read data \n",
    "melb = pd.read_csv(\"data/melb_training_data.csv\").sample(frac=1, random_state=randomState).reset_index(drop=True)\n",
    "melb.drop(columns = [\"Unnamed: 0\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cardiac-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "## select the features\n",
    "features_in_model = [\"Distance\", \"Rooms\", \"YearBuilt\", \"BuildingArea\",\"Landsize\"]\n",
    "X = melb[features_in_model]\n",
    "\n",
    "## set the target\n",
    "y = melb[\"Price\"]\n",
    "\n",
    "## Train / test split\n",
    "X_train = X[:9000]\n",
    "y_train = y[:9000]\n",
    "X_test = X[9000:]\n",
    "y_test = y[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "alternative-object",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the no-feature-engineering data (missing value only):\n",
      "R^2 on training data = 0.9404\n",
      "R^2 on test data = 0.5802\n"
     ]
    }
   ],
   "source": [
    "## Random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators = 300, \n",
    "                                bootstrap = True,\n",
    "                                 oob_score = True,\n",
    "                                 n_jobs = 3,\n",
    "                                 random_state = randomState)\n",
    "\n",
    "print(\"With the no-feature-engineering data (missing value only):\")\n",
    "rf_model.fit(X_train.fillna(-1), y_train)\n",
    "\n",
    "print(\"R^2 on training data = {:.4f}\".format(rf_model.score(X_train.fillna(-1), \n",
    "                                                            y_train)))\n",
    "print(\"R^2 on test data = {:.4f}\".format(rf_model.score(X_test.fillna(-1), \n",
    "                                                        y_test)))\n",
    "\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e65f978",
   "metadata": {},
   "source": [
    "## 3-2 GBDT\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "personal-works",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the no-feature-engineering data (missing value only):\n",
      "R^2 on training data = 0.6422\n",
      "R^2 on test data = 0.5884\n"
     ]
    }
   ],
   "source": [
    "## GBDT, vanilla\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb_model = GradientBoostingRegressor()\n",
    "\n",
    "print(\"With the no-feature-engineering data (missing value only):\")\n",
    "gb_model.fit(X_train.fillna(-1), y_train)\n",
    "\n",
    "print(\"R^2 on training data = {:.4f}\".format(gb_model.score(X_train.fillna(-1), \n",
    "                                                            y_train)))\n",
    "print(\"R^2 on test data = {:.4f}\".format(gb_model.score(X_test.fillna(-1), \n",
    "                                                        y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bc9d65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the default number of trees is 100\n",
    "## however, it might not be optimal\n",
    "gb_model.n_estimators_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c2a21e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the no-feature-engineering data (missing value only):\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1 359383192792.4924 48237920154.5913            3.18s\n",
      "         2 323517146880.5162 37909916499.0505            3.00s\n",
      "         3 283179143652.9827 30349059518.7709            2.88s\n",
      "         4 264137927562.2318 23829249294.3306            2.73s\n",
      "         5 239879445410.0410 20886438962.6912            2.61s\n",
      "         6 224192665727.5069 15611005011.8634            2.67s\n",
      "         7 212853940556.6908 12591651987.7530            2.78s\n",
      "         8 190845765454.6474  9898365846.4881            2.86s\n",
      "         9 172846627013.3734  7652812919.4645            2.86s\n",
      "        10 163290741856.3474  6414248729.8679            2.87s\n",
      "        20 110744216247.7794   812114792.8341            2.53s\n",
      "        30 94294016016.6601   -88566919.5381            3.32s\n",
      "        40 85106438253.6104    -3984027.0072            2.91s\n",
      "        50 73424017675.4474   -74776817.9641            2.52s\n",
      "        60 65742155718.6931  -114628184.0228            2.24s\n",
      "        70 66126791967.0744  -135038827.4910            2.05s\n",
      "        80 57744113105.5047  -299992147.5460            1.92s\n",
      "        90 57538700162.3487  -290563757.7956            1.78s\n",
      "       100 54782967431.1934  -341399125.6322            1.66s\n",
      "R^2 on training data = 0.8503\n",
      "R^2 on test data = 0.6254\n"
     ]
    }
   ],
   "source": [
    "## almost the same hyperparameter manner with Random forest\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    ## number of trees, NOT the more the better\n",
    "    n_estimators = 300, \n",
    "    ## these two controls the size of a tree = prune\n",
    "    max_depth = 10, \n",
    "    min_samples_leaf = 5, \n",
    "    ## how many features can a decision tree use?\n",
    "    max_features = \"sqrt\",\n",
    "    ## by default gbdt dont do bootstrap sampling\n",
    "    ## subsample less than 1.0 enables sampling\n",
    "    subsample = 0.7,\n",
    "    ## with enabling subsample, gbdt can also have oob (not score though)\n",
    "    ## since gbdt \"improves\" tree by tree\n",
    "    ## if there is no significant improvement, the whole model can stop before all trees generated\n",
    "    n_iter_no_change = 10,\n",
    "    ## leaning rate\n",
    "    learning_rate = 0.1, \n",
    "    ## showing messeges\n",
    "    verbose = 1,\n",
    "    random_state = randomState\n",
    ")\n",
    "\n",
    "print(\"With the no-feature-engineering data (missing value only):\")\n",
    "gb_model.fit(X_train.fillna(-1), y_train)\n",
    "\n",
    "print(\"R^2 on training data = {:.4f}\".format(gb_model.score(X_train.fillna(-1), \n",
    "                                                            y_train)))\n",
    "print(\"R^2 on test data = {:.4f}\".format(gb_model.score(X_test.fillna(-1), \n",
    "                                                        y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "varied-requirement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## I've previously specified 300 trees for this model\n",
    "## However, it stop with 106 trees\n",
    "## Since scikit learn finds that there is no further improvement with more than 106 trees\n",
    "\n",
    "gb_model.n_estimators_\n",
    "\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ranging-credit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Distance</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>BuildingArea</th>\n",
       "      <th>Landsize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>6.4000</td>\n",
       "      <td>2</td>\n",
       "      <td>1990.0000</td>\n",
       "      <td>89.0000</td>\n",
       "      <td>312.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Distance  Rooms  YearBuilt  BuildingArea  Landsize\n",
       "9000    6.4000      2  1990.0000       89.0000  312.0000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Take look the first row in test data \n",
    "\n",
    "one_line_test = X_test.head(1).fillna(-1)\n",
    "\n",
    "one_line_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "historic-burns",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicton from RF = 728720.0000\n",
      "Predicton from GBDT = 770003.7545\n",
      "True value = 468000.0000\n"
     ]
    }
   ],
   "source": [
    "## And the predicted values from random forest and GBDT\n",
    "print(\"Predicton from RF = {:.4f}\".format(rf_model.predict(one_line_test)[0]))\n",
    "print(\"Predicton from GBDT = {:.4f}\".format(gb_model.predict(one_line_test)[0]))\n",
    "print(\"True value = {:.4f}\".format(y_test.values[0]))\n",
    "\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "delayed-vanilla",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([612500.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Take the first decision tree in the random forest I've trained\n",
    "rf_model.estimators_[0].predict(one_line_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "congressional-indie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "728720.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Take all the prediction results in random forest\n",
    "rf_results = list()\n",
    "\n",
    "for i in range(len(rf_model.estimators_)):\n",
    "    rf_results.append(rf_model.estimators_[i].predict(one_line_test))\n",
    "\n",
    "## The RF take the average of the results from all the trees\n",
    "np.mean(rf_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "threaded-squad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([770003.75446595])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## On the other hand, GBDT add all results up\n",
    "gb_results = list()\n",
    "\n",
    "for i in range(len(gb_model.estimators_)):\n",
    "    gb_results.append(gb_model.estimators_[i][0].predict(one_line_test))\n",
    "\n",
    "## gb_model.init_ is the mean(y) from training data as the 0-th prediction\n",
    "## the learning_rate controls how fast the results are added up\n",
    "\n",
    "gb_model.init_.predict(one_line_test) + np.sum(gb_results) * gb_model.learning_rate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-scratch",
   "metadata": {},
   "source": [
    "## 3-3 More evaluation: GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "occupational-observation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: R^2 on test data = 0.5802\n",
      "GBDT: R^2 on test data = 0.6254\n"
     ]
    }
   ],
   "source": [
    "## If I have only few models, it's ok to use test R2 for choosing the best model\n",
    "\n",
    "print(\"Random Forest: R^2 on test data = {:.4f}\".format(rf_model.score(X_test.fillna(-1), \n",
    "                                                        y_test)))\n",
    "\n",
    "print(\"GBDT: R^2 on test data = {:.4f}\".format(gb_model.score(X_test.fillna(-1), \n",
    "                                                        y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "collect-politics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01 0.07 0.13 0.19 0.25]\n",
      "[ 1  5  9 13 17]\n"
     ]
    }
   ],
   "source": [
    "## I want to find a good set of hyperparameters\n",
    "## Take GBDT for example, the hyperparameters to choose is learning_rate & min_samples_leaf\n",
    "\n",
    "learning_rates = np.arange(0.01, 0.3, 0.06)\n",
    "min_samples_leafs = np.arange(1, 20, 4)\n",
    "\n",
    "## 5 values for both => number of models = 25\n",
    "## this is grid search\n",
    "print(learning_rates)\n",
    "print(min_samples_leafs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cheap-peoples",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0.01, 1): 0.5256142302658419,\n",
       " (0.01, 5): 0.5250243798008423,\n",
       " (0.01, 9): 0.5236738947853083,\n",
       " (0.01, 13): 0.5229067341597174,\n",
       " (0.01, 17): 0.5228485455369929,\n",
       " (0.06999999999999999, 1): 0.6045845572552284,\n",
       " (0.06999999999999999, 5): 0.5693352731126812,\n",
       " (0.06999999999999999, 9): 0.5696395016397571,\n",
       " (0.06999999999999999, 13): 0.5975864041706282,\n",
       " (0.06999999999999999, 17): 0.5940958065635806,\n",
       " (0.12999999999999998, 1): 0.5782171638256237,\n",
       " (0.12999999999999998, 5): 0.5668037781274522,\n",
       " (0.12999999999999998, 9): 0.5758478888745222,\n",
       " (0.12999999999999998, 13): 0.5930712179623087,\n",
       " (0.12999999999999998, 17): 0.5977175520732401,\n",
       " (0.18999999999999997, 1): 0.6002485635061221,\n",
       " (0.18999999999999997, 5): 0.5586668459807924,\n",
       " (0.18999999999999997, 9): 0.6095951724817985,\n",
       " (0.18999999999999997, 13): 0.5877448245871291,\n",
       " (0.18999999999999997, 17): 0.6119960338501851,\n",
       " (0.24999999999999997, 1): 0.6085056003082734,\n",
       " (0.24999999999999997, 5): 0.6018058035253546,\n",
       " (0.24999999999999997, 9): 0.5982254605642026,\n",
       " (0.24999999999999997, 13): 0.5714213494779046,\n",
       " (0.24999999999999997, 17): 0.6062166258595391}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## try all the combinations\n",
    "hyper_searching_results = dict()\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for mnl in min_samples_leafs:\n",
    "        #print(lr, mnl)\n",
    "        gb_model = GradientBoostingRegressor(n_estimators = 300, \n",
    "                                     subsample = 0.7,\n",
    "                                    n_iter_no_change = 10,\n",
    "                                     learning_rate = lr, \n",
    "                                     min_samples_leaf = mnl,\n",
    "                                     random_state = randomState)\n",
    "\n",
    "        gb_model.fit(X_train.fillna(-1), y_train)\n",
    "        R2_test = gb_model.score(X_test.fillna(-1), y_test)\n",
    "        hyper_searching_results[(lr, mnl)] = R2_test\n",
    "        \n",
    "hyper_searching_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fifteen-premises",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.18999999999999997, 17)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the winner goes to ......\n",
    "max(hyper_searching_results, key=hyper_searching_results.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-genesis",
   "metadata": {},
   "source": [
    "### 3-3-1 Cross Validation & Hyperparameter tuning\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "virgin-medicine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=GradientBoostingRegressor(n_estimators=300,\n",
       "                                                 n_iter_no_change=10,\n",
       "                                                 random_state=8,\n",
       "                                                 subsample=0.7),\n",
       "             param_grid={'learning_rate': array([0.01, 0.07, 0.13, 0.19, 0.25]),\n",
       "                         'min_samples_leaf': array([ 1,  5,  9, 13, 17])})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## in many applications, we don't abuse test set like that\n",
    "## we use cross validation to replace multiple evaluations on test set\n",
    "\n",
    "## to do CV, there is no need to write multiple loops all by myself\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters_to_search = {'learning_rate': learning_rates, \n",
    "              'min_samples_leaf': min_samples_leafs}\n",
    "\n",
    "gb_model = GradientBoostingRegressor(n_estimators = 300, \n",
    "                                     subsample = 0.7,\n",
    "                                    n_iter_no_change = 10,\n",
    "                                     random_state = randomState)\n",
    "\n",
    "gb_model_CV = GridSearchCV(gb_model, parameters_to_search, cv=5)\n",
    "gb_model_CV.fit(X_train.fillna(-1), y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "instrumental-pharmacy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(n_estimators=300, n_iter_no_change=10, random_state=8,\n",
       "                          subsample=0.7)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "rough-quest",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([1.10115061, 1.0822782 , 1.16300435, 1.20252867, 1.13190556,\n",
       "        0.93695707, 1.02462115, 0.91159801, 0.81108298, 1.07546406,\n",
       "        0.58428254, 0.63250751, 0.56864076, 0.61682415, 0.59642744,\n",
       "        0.42520294, 0.52079682, 0.25295877, 0.44346762, 0.34303317,\n",
       "        0.32421279, 0.31541018, 0.28107686, 0.30497894, 0.36360698]),\n",
       " 'std_fit_time': array([0.0477549 , 0.01121318, 0.03622197, 0.03729014, 0.02644122,\n",
       "        0.2142103 , 0.27748432, 0.16012781, 0.0973081 , 0.38091056,\n",
       "        0.13281074, 0.23502588, 0.22064236, 0.11447493, 0.14402936,\n",
       "        0.15947318, 0.10874454, 0.04508488, 0.11823116, 0.0747882 ,\n",
       "        0.13355745, 0.10681572, 0.08781629, 0.10895786, 0.17252945]),\n",
       " 'mean_score_time': array([0.0112659 , 0.01158242, 0.01183805, 0.01230783, 0.01148973,\n",
       "        0.0080759 , 0.00891318, 0.00798092, 0.00743284, 0.01003289,\n",
       "        0.00620513, 0.00633316, 0.0059432 , 0.00604124, 0.00569816,\n",
       "        0.00445518, 0.00503469, 0.00378184, 0.0046134 , 0.00407476,\n",
       "        0.00408087, 0.00420227, 0.00360694, 0.00379   , 0.00405831]),\n",
       " 'std_score_time': array([0.00012184, 0.00086302, 0.00059278, 0.00068451, 0.00078854,\n",
       "        0.00164904, 0.00168977, 0.00104855, 0.00065217, 0.00360864,\n",
       "        0.00076857, 0.00164224, 0.00179137, 0.00050112, 0.00083148,\n",
       "        0.00107113, 0.00072947, 0.00014987, 0.00054495, 0.00039556,\n",
       "        0.00101623, 0.00078176, 0.00041686, 0.00041951, 0.00116224]),\n",
       " 'param_learning_rate': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.06999999999999999,\n",
       "                    0.06999999999999999, 0.06999999999999999,\n",
       "                    0.06999999999999999, 0.06999999999999999,\n",
       "                    0.12999999999999998, 0.12999999999999998,\n",
       "                    0.12999999999999998, 0.12999999999999998,\n",
       "                    0.12999999999999998, 0.18999999999999997,\n",
       "                    0.18999999999999997, 0.18999999999999997,\n",
       "                    0.18999999999999997, 0.18999999999999997,\n",
       "                    0.24999999999999997, 0.24999999999999997,\n",
       "                    0.24999999999999997, 0.24999999999999997,\n",
       "                    0.24999999999999997],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_leaf': masked_array(data=[1, 5, 9, 13, 17, 1, 5, 9, 13, 17, 1, 5, 9, 13, 17, 1,\n",
       "                    5, 9, 13, 17, 1, 5, 9, 13, 17],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'learning_rate': 0.01, 'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.01, 'min_samples_leaf': 5},\n",
       "  {'learning_rate': 0.01, 'min_samples_leaf': 9},\n",
       "  {'learning_rate': 0.01, 'min_samples_leaf': 13},\n",
       "  {'learning_rate': 0.01, 'min_samples_leaf': 17},\n",
       "  {'learning_rate': 0.06999999999999999, 'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.06999999999999999, 'min_samples_leaf': 5},\n",
       "  {'learning_rate': 0.06999999999999999, 'min_samples_leaf': 9},\n",
       "  {'learning_rate': 0.06999999999999999, 'min_samples_leaf': 13},\n",
       "  {'learning_rate': 0.06999999999999999, 'min_samples_leaf': 17},\n",
       "  {'learning_rate': 0.12999999999999998, 'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.12999999999999998, 'min_samples_leaf': 5},\n",
       "  {'learning_rate': 0.12999999999999998, 'min_samples_leaf': 9},\n",
       "  {'learning_rate': 0.12999999999999998, 'min_samples_leaf': 13},\n",
       "  {'learning_rate': 0.12999999999999998, 'min_samples_leaf': 17},\n",
       "  {'learning_rate': 0.18999999999999997, 'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.18999999999999997, 'min_samples_leaf': 5},\n",
       "  {'learning_rate': 0.18999999999999997, 'min_samples_leaf': 9},\n",
       "  {'learning_rate': 0.18999999999999997, 'min_samples_leaf': 13},\n",
       "  {'learning_rate': 0.18999999999999997, 'min_samples_leaf': 17},\n",
       "  {'learning_rate': 0.24999999999999997, 'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.24999999999999997, 'min_samples_leaf': 5},\n",
       "  {'learning_rate': 0.24999999999999997, 'min_samples_leaf': 9},\n",
       "  {'learning_rate': 0.24999999999999997, 'min_samples_leaf': 13},\n",
       "  {'learning_rate': 0.24999999999999997, 'min_samples_leaf': 17}],\n",
       " 'split0_test_score': array([0.53704674, 0.53768311, 0.53688407, 0.53867882, 0.53885884,\n",
       "        0.62850862, 0.62295884, 0.6295102 , 0.62352542, 0.61908101,\n",
       "        0.62612779, 0.61585446, 0.62840244, 0.63433274, 0.61832994,\n",
       "        0.6296768 , 0.63767335, 0.62360998, 0.62316547, 0.61488541,\n",
       "        0.63664399, 0.60811793, 0.62894897, 0.62134864, 0.61835719]),\n",
       " 'split1_test_score': array([0.55053791, 0.55132319, 0.55118466, 0.55045567, 0.55084991,\n",
       "        0.62324577, 0.62588715, 0.62343955, 0.62754768, 0.62611151,\n",
       "        0.61874685, 0.62882   , 0.6258059 , 0.62295609, 0.6261618 ,\n",
       "        0.61595208, 0.63681673, 0.61931953, 0.64973663, 0.61530651,\n",
       "        0.61075624, 0.63382376, 0.63125458, 0.60996942, 0.61595407]),\n",
       " 'split2_test_score': array([0.48423903, 0.48485553, 0.48434539, 0.48287154, 0.48124749,\n",
       "        0.57128997, 0.56881087, 0.56157739, 0.55264186, 0.55005107,\n",
       "        0.57348503, 0.56733162, 0.54661809, 0.56266101, 0.55921425,\n",
       "        0.55846855, 0.57294632, 0.54348942, 0.5471791 , 0.5599212 ,\n",
       "        0.56345595, 0.58118474, 0.55479905, 0.54125591, 0.56505703]),\n",
       " 'split3_test_score': array([0.56140281, 0.5610637 , 0.55997057, 0.56002677, 0.55948329,\n",
       "        0.61791857, 0.63609296, 0.63639668, 0.63404559, 0.63452994,\n",
       "        0.61784744, 0.64249873, 0.64499635, 0.63512331, 0.63341132,\n",
       "        0.61294069, 0.63964024, 0.62366926, 0.62998569, 0.62895929,\n",
       "        0.6009921 , 0.62531627, 0.62339398, 0.62274085, 0.63954031]),\n",
       " 'split4_test_score': array([0.55893328, 0.55953631, 0.55859399, 0.55872071, 0.55725772,\n",
       "        0.63847883, 0.63858159, 0.63047517, 0.62757293, 0.62160616,\n",
       "        0.62688697, 0.63435846, 0.62030929, 0.63667303, 0.62006655,\n",
       "        0.63400138, 0.63790843, 0.60853634, 0.62936123, 0.62551743,\n",
       "        0.62985689, 0.64017628, 0.61559711, 0.62520008, 0.62895052]),\n",
       " 'mean_test_score': array([0.53843195, 0.53889237, 0.53819574, 0.5381507 , 0.53753945,\n",
       "        0.61588835, 0.61846628, 0.6162798 , 0.61306669, 0.61027594,\n",
       "        0.61261881, 0.61777265, 0.61322641, 0.61834923, 0.61143677,\n",
       "        0.6102079 , 0.62499701, 0.60372491, 0.61588563, 0.60891797,\n",
       "        0.60834103, 0.6177238 , 0.61079874, 0.60410298, 0.61357182]),\n",
       " 'std_test_score': array([0.0284018 , 0.02826201, 0.02814436, 0.02866763, 0.02904492,\n",
       "        0.02331038, 0.02551955, 0.02765811, 0.03039997, 0.03056783,\n",
       "        0.01991189, 0.02667017, 0.03430662, 0.02826661, 0.02664165,\n",
       "        0.02706508, 0.0260415 , 0.03061976, 0.03549613, 0.02511694,\n",
       "        0.02583989, 0.02120539, 0.02851491, 0.03185605, 0.0256637 ]),\n",
       " 'rank_test_score': array([22, 21, 23, 24, 25,  7,  2,  6, 11, 15, 12,  4, 10,  3, 13, 16,  1,\n",
       "        20,  8, 17, 18,  5, 14, 19,  9], dtype=int32)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the gridcv module run the models and save the results for us\n",
    "gb_model_CV.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dirty-mineral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.53843195, 0.53889237, 0.53819574, 0.5381507 , 0.53753945,\n",
       "       0.61588835, 0.61846628, 0.6162798 , 0.61306669, 0.61027594,\n",
       "       0.61261881, 0.61777265, 0.61322641, 0.61834923, 0.61143677,\n",
       "       0.6102079 , 0.62499701, 0.60372491, 0.61588563, 0.60891797,\n",
       "       0.60834103, 0.6177238 , 0.61079874, 0.60410298, 0.61357182])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the mean of 5-folds test(not true test) R2\n",
    "gb_model_CV.cv_results_[\"mean_test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cardiac-acrylic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.18999999999999997, min_samples_leaf=5,\n",
       "                          n_estimators=300, n_iter_no_change=10, random_state=8,\n",
       "                          subsample=0.7)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the best one is learning_rate=0.18999999999999997, min_samples_leaf=5\n",
    "gb_model_CV.best_estimator_\n",
    "\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-prefix",
   "metadata": {},
   "source": [
    "## 3-4 Learn from models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-posting",
   "metadata": {},
   "source": [
    "### 3-4-1 Feature importance\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html?highlight=randomforestregressor#sklearn.ensemble.RandomForestRegressor.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "signed-revision",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3028007 , 0.23719954, 0.0910567 , 0.14050044, 0.22844263])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## It's easy to get feature inportance from tree-based models\n",
    "## the values is normalized variation reduction\n",
    "## sum of them is 1\n",
    "\n",
    "rf_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "finite-degree",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Distance', 'Rooms', 'YearBuilt', 'BuildingArea', 'Landsize']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The order corresponds to the features you put in the model\n",
    "features_in_model\n",
    "\n",
    "## so distance is important when predicting house price\n",
    "## but is this equivalent to the answer from linear model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "stupid-guest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear : R^2 on test data = 0.3956\n"
     ]
    }
   ],
   "source": [
    "## for linear models\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train_lr = X_train.copy()\n",
    "X_test_lr = X_test.copy()\n",
    "\n",
    "## missing values\n",
    "mean_YearBuilt = X_train.YearBuilt.mean()\n",
    "mean_BuildingArea = X_train.BuildingArea.mean()\n",
    "\n",
    "X_train_lr[\"YearBuilt\"] = X_train[\"YearBuilt\"].fillna(mean_YearBuilt)\n",
    "X_train_lr[\"BuildingArea\"] = X_train[\"BuildingArea\"].fillna(mean_BuildingArea)\n",
    "\n",
    "X_test_lr[\"YearBuilt\"] = X_test_lr[\"YearBuilt\"].fillna(mean_YearBuilt)\n",
    "X_test_lr[\"BuildingArea\"] = X_test_lr[\"BuildingArea\"].fillna(mean_BuildingArea)\n",
    "\n",
    "\n",
    "## train without any transformation\n",
    "lr_model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "## train the Linear regression model\n",
    "lr_model.fit(X_train_lr, y_train)\n",
    "\n",
    "print(\"Linear : R^2 on test data = {:.4f}\".format(lr_model.score(X_test_lr, \n",
    "                                                        y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "rough-contract",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient for Distance = -32130.3773\n",
      "Coefficient for Rooms = 377407.7353\n",
      "Coefficient for YearBuilt = -4158.6728\n",
      "Coefficient for BuildingArea = 45.7759\n",
      "Coefficient for Landsize = 3.8707\n"
     ]
    }
   ],
   "source": [
    "## the coefficents\n",
    "for i in range(len(features_in_model)):\n",
    "    print(\"Coefficient for {} = {:.4f}\".format(features_in_model[i], lr_model.coef_[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "simplified-strategy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear : R^2 on test data = 0.3956\n",
      "Coefficient for Distance = -32130.3773\n",
      "Coefficient for Rooms = 377407.7353\n",
      "Coefficient for YearBuilt = -4158.6728\n",
      "Coefficient for BuildingArea = 45775.9234\n",
      "Coefficient for Landsize = 3.8707\n"
     ]
    }
   ],
   "source": [
    "## if the unit of some variables changes, the corresponding coefficient also alters\n",
    "\n",
    "X_train_lr[\"BuildingArea\"] = X_train_lr[\"BuildingArea\"] / 1000\n",
    "X_test_lr[\"BuildingArea\"] = X_test_lr[\"BuildingArea\"] / 1000\n",
    "\n",
    "## train without any transformation\n",
    "lr_model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "## train the Linear regression model\n",
    "lr_model.fit(X_train_lr, y_train)\n",
    "print(\"Linear : R^2 on test data = {:.4f}\".format(lr_model.score(X_test_lr, \n",
    "                                                        y_test)))\n",
    "\n",
    "## the coefficents remain the same except the one for BuildingArea\n",
    "## the one for BuildingArea is 1000x larger\n",
    "for i in range(len(features_in_model)):\n",
    "    print(\"Coefficient for {} = {:.4f}\".format(features_in_model[i], lr_model.coef_[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efficient-brighton",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear : R^2 on test data = 0.3956\n",
      "Coefficient for Distance = -188170.9616\n",
      "Coefficient for Rooms = 361911.0437\n",
      "Coefficient for YearBuilt = -117549.6360\n",
      "Coefficient for BuildingArea = 21882.1181\n",
      "Coefficient for Landsize = 18586.9919\n"
     ]
    }
   ],
   "source": [
    "## Standardization\n",
    "## https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "## convert x to (x - mean(x)) / sd(x)\n",
    "## the \"unit\" of x is \"one standard deviation from mean\"\n",
    "## so this can be compared between all features\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train_lr)\n",
    "X_train_lr = scaler.transform(X_train_lr)\n",
    "X_test_lr = scaler.transform(X_test_lr)\n",
    "\n",
    "## train the Linear regression model\n",
    "lr_model = LinearRegression(fit_intercept=True)\n",
    "lr_model.fit(X_train_lr, y_train)\n",
    "print(\"Linear : R^2 on test data = {:.4f}\".format(lr_model.score(X_test_lr, \n",
    "                                                        y_test)))\n",
    "\n",
    "## Now the meaning of coefficients is \"the delta of y by 1 standard deviation of x from mean\"\n",
    "## compare the result to feature importance derived from random forest\n",
    "for i in range(len(features_in_model)):\n",
    "    print(\"Coefficient for {} = {:.4f}\".format(features_in_model[i], lr_model.coef_[i]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-tulsa",
   "metadata": {},
   "source": [
    "### 3-4-2 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "brown-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For tree-based models, you can just drop the features with low importance values\n",
    "## For linear models, you can drop features with the results from tree-base models\n",
    "\n",
    "## unlike the tree-based model, drops \"useless features\" from linear models affects results\n",
    "## you can treat \"feature combination\" just like \"hyperparameter combination\" in a CV manner\n",
    "## however, it's not a \"Grid search\" task\n",
    "## you should use sklearn.model_selection.cross_validate to do this\n",
    "## the grammar are similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-stopping",
   "metadata": {},
   "source": [
    "### 3-5 What is a good model (for you)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e15a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a12da23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
